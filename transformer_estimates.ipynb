{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21213552-eba3-426b-ae8b-2529d6c8ddf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be58a90-081f-48f5-adec-be236a190106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matmul import linear_estimates, logit_estimates, attend_estimates\n",
    "from norm import layer_norm_estimates\n",
    "from pointwise import softmax_estimates, dropout_estimates, nonlinear_act_estimates\n",
    "from time_projections import get_time_flops, get_time_mem, get_time_comm, get_topology, get_total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36c909a-afa6-46ab-b887-e7182bd35c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_timings_and_stats(summary, system):\n",
    "    ''' timings, any other df stats '''\n",
    "\n",
    "    # which layers use tensor cores\n",
    "    tensor_core_layers = ['fc1', 'fc2', 'qkv_proj', 'v_proj', 'logits', 'attend']\n",
    "   \n",
    "    # time for forward\n",
    "    summary['t_comp_fwd'] = summary.apply(lambda x: get_time_flops(x[\"flops_fwd\"], \n",
    "                                                                   use_tensor=(x[\"layer\"] in tensor_core_layers),\n",
    "                                                                   system=system), axis=1)\n",
    "    summary['t_mem_fwd'] = summary.apply(lambda x: get_time_mem(x[\"total_mem_fwd\"], system=system), axis=1)\n",
    "    # time for backward\n",
    "    summary['t_comp_bwd'] = summary.apply(lambda x: get_time_flops(x[\"flops_bwd\"], \n",
    "                                                                   use_tensor=(x[\"layer\"] in tensor_core_layers),\n",
    "                                                                   system=system), axis=1)\n",
    "    summary['t_mem_bwd'] = summary.apply(lambda x: get_time_mem(x[\"total_mem_bwd\"], system=system), axis=1)\n",
    "    \n",
    "    # times\n",
    "    summary['intensity'] = summary['t_comp_fwd'] / summary['t_mem_fwd']\n",
    "    # roofline\n",
    "    summary['t_fwd'] = summary.apply(lambda x: max(x['t_comp_fwd'], x['t_mem_fwd']), axis=1)\n",
    "    summary['t_bwd'] = summary.apply(lambda x: max(x['t_comp_bwd'], x['t_mem_bwd']), axis=1)\n",
    "    \n",
    "    # time for communication\n",
    "    use_empirical = False\n",
    "    summary['comm_topology'] = summary.apply(lambda x: get_topology(x[\"comm_size\"], system=system), axis=1)\n",
    "    summary['t_comm_fwd'] = summary.apply(lambda x: get_time_comm(x[\"comm_fwd\"],\n",
    "                                                                  n_gpus=x[\"comm_size\"],\n",
    "                                                                  comm_type=x[\"comm_fwd_type\"], \n",
    "                                                                  topology=x[\"comm_topology\"],\n",
    "                                                                  empirical=use_empirical,\n",
    "                                                                  system=system), axis=1)\n",
    "    summary['t_comm_bwd'] = summary.apply(lambda x: get_time_comm(x[\"comm_bwd\"],\n",
    "                                                                  n_gpus=x[\"comm_size\"],\n",
    "                                                                  comm_type=x[\"comm_bwd_type\"], \n",
    "                                                                  topology=x[\"comm_topology\"],\n",
    "                                                                  empirical=use_empirical,\n",
    "                                                                  system=system), axis=1)\n",
    "    \n",
    "    # total time\n",
    "    summary['t_total_fwd'] = summary.apply(lambda x: get_total_time(x['t_fwd'], x['t_comm_fwd']), axis=1)\n",
    "    summary['t_total_bwd'] = summary.apply(lambda x: get_total_time(x['t_bwd'], x['t_comm_bwd']), axis=1)\n",
    "    \n",
    "    # fraction\n",
    "    summary['frac_t_comm_fwd'] = summary['t_comm_fwd'] / summary['t_total_fwd']\n",
    "    summary['frac_t_comm_bwd'] = summary['t_comm_bwd'] / summary['t_total_bwd']\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bc19b5-3714-4e49-9429-a81a2011204d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### nn modules ###\n",
    "def MLP_estimates(b, l, e, f, element_size=4E-6, mask_element_size=1E-6, flops_units=1E-12, \n",
    "                  parallelism={'sequence' : 1,\n",
    "                               'tensor': {'1D': 1, '2D': 0, '2.5D': 0,'3D': 0}}, system={}):\n",
    "    \"\"\"\n",
    "    MLP layer estimates\n",
    "    parameters: b: batch size\n",
    "                l: seq length\n",
    "                e: embedding dim\n",
    "                f: hidden dim\n",
    "                element_size: in MB\n",
    "                mask_element_size: in MB (for dropout)\n",
    "    \n",
    "    tensor shapes: input tensor: (b,l,e)\n",
    "                   output tensor: (b,l,e)\n",
    "                   \n",
    "    layer arithmetic: \n",
    "        forward pass: \n",
    "             X = Norm(X)\n",
    "             X = XW + b\n",
    "             (b,l,f) = (b,l,e) * (e,f) + (1,f)\n",
    "             X = nonlinear(X)\n",
    "             (b,l,f) = (b,l,f)\n",
    "             X = dropout(X)\n",
    "             (b,l,f) = (b,l,f) * (b,l,f) [random mask]\n",
    "             X = linear(X)\n",
    "             (b,l,e) = (b,l,f) * (f,e) + (1,e)\n",
    "             X = dropout(X)\n",
    "             (b,l,e) = (b,l,e) * (b,l,e) [random mask]\n",
    "            \n",
    "        backward pass:\n",
    "             chain rule\n",
    "             \n",
    "    parallelism:\n",
    "            sequence: a integer grater than or equal to 1, must divide l\n",
    "                      TODO: check ifit changes depending on tensor parallelism\n",
    "    \n",
    "            tensor:\n",
    "                if 1D>0, higher orders are ignored\n",
    "                if 1D==0, check for 2D, if 2D==0, go to next and so on\n",
    "                2D ->[m1,m2], 2.5D and 3D ->[m1,m2,m3]\n",
    "            \n",
    "                1D:\n",
    "                    X = Norm(X)\n",
    "                    X = XW + b\n",
    "                    (b,l,f/m) = (b,l,e) * (e,f/m) + (1,f/m)\n",
    "                    X = nonlinear(X)\n",
    "                    (b,l,f/m) = (b,l,f/m)\n",
    "                    X = dropout(X)\n",
    "                    (b,l,f/m) = (b,l,f/m) * (b,l,f/m) [random mask]\n",
    "                    X = linear(X)\n",
    "                    (b,l,e/m) = (b,l,f/m) * (f/m,e) + (1,e)\n",
    "                    X = dropout(X)\n",
    "                    (b,l,e) = (b,l,e) * (b,l,e) [random mask]\n",
    "                2D:   TODO complete\n",
    "                2.5D: TODO complete\n",
    "                3D:   TODO complete\n",
    "            \n",
    "    comments: \n",
    "    \"\"\"\n",
    "    \n",
    "    summary = []\n",
    "    \n",
    "    flops_per_add = 1 * flops_units\n",
    "    \n",
    "    # fwd allgather comms if sequence parallelsim >1\n",
    "    if parallelism['sequence']>1:\n",
    "        s1=parallelism['sequence']\n",
    "            \n",
    "        stats = layer_norm_estimates(b,l//s1,e,element_size=element_size,flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"layer_norm_1\"\n",
    "            \n",
    "        stats[\"comm_fwd\"] = (b*l*e) * (s1-1)/s1 * element_size # bwd comms for gather from sequence parallelism\n",
    "        stats[\"comm_fwd_type\"] = \"allgather\" \n",
    "        stats[\"comm_size\"] = s1\n",
    "        summary.append(stats)\n",
    "    else:\n",
    "        stats = layer_norm_estimates(b,l,e,element_size=element_size,flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"layer_norm_1\"\n",
    "        summary.append(stats)\n",
    "        \n",
    "    \n",
    "    if parallelism['tensor']['1D'] > 0:\n",
    "        m1 = parallelism['tensor']['1D']\n",
    "        m1_parallel = (m1 > 1)\n",
    "\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "        stats = linear_estimates(b,l,e,f//m1,element_size=element_size,has_bias=True,flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"fc1\"       \n",
    "        \n",
    "        # sync/comm layers\n",
    "        \n",
    "        stats[\"comm_bwd\"] = m1_parallel * (b*l*e) * (m1-1)/m1 * element_size # bwd comms for partial sums of b,l,e\n",
    "        stats[\"comm_bwd_type\"] = \"reducescatter\" \n",
    "        stats[\"comm_size\"] = m1\n",
    "    \n",
    "        # addition computation due to reduce operation\n",
    "        stats[\"flops_bwd\"] += (b*l*e)*(m1-1)/m1 * flops_per_add\n",
    "        \n",
    "        summary.append(stats)\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = nonlinear_act_estimates(b,l,f//m1,element_size=element_size,flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"act\"\n",
    "        summary.append(stats)\n",
    "    \n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = dropout_estimates(b, l, f // m1, element_size=element_size, mask_element_size=mask_element_size, flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"dpr1\"\n",
    "        summary.append(stats)\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = linear_estimates(b,l,f//m1,e,element_size=element_size,has_bias=True,flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"fc2\"\n",
    "        # sync/comm layers\n",
    "    \n",
    "        \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "        \n",
    "        stats[\"comm_fwd\"] =  m1_parallel * (b*l*e) * (m1-1)/m1 * element_size # fwd comms for partial sums of b,l,e\n",
    "        stats[\"comm_fwd_type\"] = \"reducescatter\"\n",
    "        stats[\"comm_size\"] = m1\n",
    "        \n",
    "        # addition computation due to reduce operation\n",
    "        stats[\"flops_fwd\"] += (b*l*e)*(m1-1)/m1 * flops_per_add\n",
    "        summary.append(stats)\n",
    "    \n",
    "    \n",
    "    elif parallelism[\"tensor\"][\"2D\"] !=0:\n",
    "        assert len(parallelism[\"tensor\"][\"2D\"])==2\n",
    "        m1,m2 =parallelism[\"tensor\"][\"2D\"]\n",
    "        pass\n",
    "    \n",
    "    elif parallelism['tensor']['2.5D'] != 0:\n",
    "        assert len(parallelism[\"tensor\"][\"2.5D\"])==3\n",
    "        m1,m2,m3 =parallelism[\"tensor\"][\"2.5D\"]\n",
    "        pass\n",
    "    \n",
    "    elif parallelism['tensor']['3D'] != 0:\n",
    "        assert len(parallelism[\"tensor\"][\"3D\"])==3\n",
    "        m1,m2,m3 =parallelism[\"tensor\"][\"3D\"]\n",
    "        pass\n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "    # bwd allgather comms if sequence parallelism >1\n",
    "    if parallelism['sequence']>1:\n",
    "        s1=parallelism['sequence']\n",
    "        stats[\"comm_bwd\"] = (b*l*e) * (s1-1)/s1 * element_size # bwd comms for gather from sequence parallelism\n",
    "        stats[\"comm_bwd_type\"] = \"allgather\" \n",
    "        stats[\"comm_size\"] = s1\n",
    "            \n",
    "        stats = dropout_estimates(b,l,f//s1,element_size=element_size,mask_element_size=mask_element_size, \n",
    "                                  flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"dpr1\"\n",
    "        summary.append(stats)\n",
    "    else:\n",
    "        stats = dropout_estimates(b,l,f,element_size=element_size,mask_element_size=mask_element_size, \n",
    "                                  flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"dpr1\"\n",
    "        summary.append(stats)\n",
    "    \n",
    "    summary = pd.DataFrame(summary)\n",
    "    summary = compute_timings_and_stats(summary, system)\n",
    "    \n",
    "    return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2d41fa-3acd-4db3-b89d-da5cc2384945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_estimates(b, l, e, h, element_size=4E-6, mask_element_size=1E-6, flops_units=1E-12, \n",
    "                             parallelism={'sequence' : 1,\n",
    "                                          'tensor': {'1D': 1, '2D': 0, '2.5D': 0,'3D': 0}}, system={}):\n",
    "    \"\"\"\n",
    "    dropout layer estimates\n",
    "    parameters: b: batch size\n",
    "                l: seq length\n",
    "                e: embedding dim/hidden dim\n",
    "                h: number of attention heads\n",
    "                element_size: in MB\n",
    "    \n",
    "    tensor shapes: input tensor: (b,l,e)\n",
    "                   output tensor: (b,l,e)\n",
    "                   \n",
    "    layer arithmetic: \n",
    "        define: q = e/h -> the effective embedding dimension per attention head\n",
    "        forward pass: \n",
    "             X = norm(X)\n",
    "             Q = XW, K = XW, V = XW\n",
    "             (b,l,h,q,3) = (b,l,e) * (e,3hq)\n",
    "             A = QK'/sqrt(q)\n",
    "             (b,h,l,l) = (b,h,l,q) * (b,h,q,l)\n",
    "             A = softmax(A)\n",
    "             (b,h,l,l) = (b,h,l,l)\n",
    "             A = dpr(A)\n",
    "             Y = AV\n",
    "             (b,h,l,q) = (b,h,l,l) * (b,h,l,q)\n",
    "             Y = VW\n",
    "             (b,l,e) = (b,l,hq) * (hq,e)\n",
    "             Y = dpr(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "             Y = norm(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "             \n",
    "        backward pass:\n",
    "             chain rule\n",
    "             \n",
    "        parallelism:\n",
    "            sequence: a integer grater than or equal to 1, must divide l\n",
    "                      TODO: check ifit changes depending on tensor parallelism\n",
    "    \n",
    "            tensor:\n",
    "                if 1D>0, higher orders are ignored\n",
    "                if 1D==0, check for 2D, if 2D==0, go to next and so on\n",
    "                2D ->[m1,m2], 2.5D and 3D ->[m1,m2,m3]\n",
    "            \n",
    "            1D:\n",
    "             X = norm(X)\n",
    "             Q = XW, K = XW, V = XW\n",
    "             (b,l,h/m,q,3) = (b,l,e) * (e,3hq/m)\n",
    "             A = QK'/sqrt(q)\n",
    "             (b,h/m,l,l) = (b,h/m,l,q) * (b,h/m,q,l)\n",
    "             A = softmax(A)\n",
    "             (b,h/m,l,l) = (b,h/m,l,l)\n",
    "             A = dpr(A)\n",
    "             (b,h/m,l,l) = (b,h/m,l,l)\n",
    "             Y = AV\n",
    "             (b,h/m,l,q) = (b,h/m,l,l) * (b,h/m,l,q)\n",
    "             Y = VW\n",
    "             (b,l,e) = (b,l,hq/m) * (hq/m,e)\n",
    "             Y = dpr(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "             Y = norm(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "            \n",
    "    \n",
    "    comments: \n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    flops_per_add = 1 * flops_units\n",
    "    \n",
    "    q = e // h\n",
    "    \n",
    "    # fwd allgather comms if sequence parallelsim >1\n",
    "        if parallelism['sequence']>1:\n",
    "            s1=parallelism['sequence']\n",
    "            stats = layer_norm_estimates(b,l//s1,e,element_size=element_size,flops_units=flops_units)\n",
    "            stats[\"layer\"] = \"layer_norm_1\"\n",
    "            stats[\"comm_fwd\"] = (b*l*e) * (s1-1)/s1 * element_size # bwd comms for gather from sequence parallelism\n",
    "            stats[\"comm_fwd_type\"] = \"allgather\" \n",
    "            stats[\"comm_size\"] = s1\n",
    "            summary.append(stats)\n",
    "        else:\n",
    "            stats = layer_norm_estimates(b,l,e,element_size=element_size,flops_units=flops_units)\n",
    "            stats[\"layer\"] = \"layer_norm_1\"\n",
    "            summary.append(stats)\n",
    "    \n",
    "    if parallelism['tensor']['1D'] > 0:\n",
    "        m1 = parallelism['tensor']['1D']\n",
    "        m1_parallel = (m1 > 1)\n",
    "    \n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = linear_estimates(b, l, e, (3*e)//m1, element_size=element_size, has_bias=False, \n",
    "                                 flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"qkv_proj\"\n",
    "        # sync/comm layers: no fwd coms here\n",
    "        stats[\"comm_bwd\"] = m1_parallel * (b*l*e) *(m1-1)/m1 * element_size # reduce scatter before going to ln: TODO check?\n",
    "        stats[\"comm_bwd_type\"] = \"reducescatter\"\n",
    "        stats[\"comm_size\"] = m1\n",
    "        # addition computation due to reduce operation\n",
    "        stats[\"flops_bwd\"] += (b*l*e)*(m1-1)/m1 * flops_per_add\n",
    "        summary.append(stats)\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = logit_estimates(b, l, q, h//m1, element_size=element_size, flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"logits\"\n",
    "        summary.append(stats)\n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = softmax_estimates(b, l, h//m1, element_size=element_size, flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"softmax\"\n",
    "        summary.append(stats)\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = dropout_estimates(b, l, (l*h)//m1, element_size=element_size, mask_element_size=mask_element_size, \n",
    "                              flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"dropout_softmax\"\n",
    "        summary.append(stats)\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "\n",
    "        stats = attend_estimates(b, l, q, h//m1, element_size=element_size, flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"attend\"\n",
    "        summary.append(stats)\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "        stats = linear_estimates(b, l, (h*q) // m1, e, element_size=element_size, has_bias=True, \n",
    "                                 flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"v_proj\"\n",
    "        # sync/comm layers\n",
    "        stats[\"comm_fwd\"] = m1_parallel * (b*l*e) * (m1-1)/m1 * element_size # fwd comms for partial sums of b,l,e\n",
    "        stats[\"comm_fwd_type\"] = \"reducescatter\"\n",
    "        stats[\"comm_size\"] = m1\n",
    "        # addition computation due to reduce operation\n",
    "        stats[\"flops_bwd\"] += (b*l*e) * (m1-1)/m1 * flops_per_add\n",
    "        summary.append(stats)\n",
    "    \n",
    "    elif parallelism[\"tensor\"][\"2D\"] !=0:\n",
    "        assert len(parallelism[\"tensor\"][\"2D\"])==2\n",
    "        m1,m2 =parallelism[\"tensor\"][\"2D\"]\n",
    "        pass\n",
    "    \n",
    "    elif parallelism['tensor']['2.5D'] != 0:\n",
    "        assert len(parallelism[\"tensor\"][\"2.5D\"])==3\n",
    "        m1,m2,m3 =parallelism[\"tensor\"][\"2.5D\"]\n",
    "        pass\n",
    "    \n",
    "    elif parallelism['tensor']['3D'] != 0:\n",
    "        assert len(parallelism[\"tensor\"][\"3D\"])==3\n",
    "        m1,m2,m3 =parallelism[\"tensor\"][\"3D\"]\n",
    "        pass\n",
    "    \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "    # bwd allgather comms if sequence parallelism >1\n",
    "    if parallelism['sequence']>1:\n",
    "        s1=parallelism['sequence']\n",
    "    \n",
    "        stats = dropout_estimates(b, l//s1, e, element_size=element_size, mask_element_size=mask_element_size, \n",
    "                                  flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"dropout\"\n",
    "        # sync/comm layers\n",
    "        stats[\"comm_bwd\"] = m1_parallel * (b*l*e) * (s1-1)/s1 * element_size\n",
    "        stats[\"comm_bwd_type\"] = \"allgather\"\n",
    "        stats[\"comm_size\"] = m1\n",
    "        summary.append(stats)\n",
    "    else:\n",
    "        stats = dropout_estimates(b, l, e, element_size=element_size, mask_element_size=mask_element_size, \n",
    "                                  flops_units=flops_units)\n",
    "        stats[\"layer\"] = \"dropout\"\n",
    "        summary.append(stats)\n",
    "        \n",
    "    ######################################################################################################################################################\n",
    "    ######################################################################################################################################################\n",
    "    \n",
    "    summary = pd.DataFrame(summary)\n",
    "    summary = compute_timings_and_stats(summary, system)\n",
    "\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "625fd0e1-403d-4fd5-a5dc-472832d0b039",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: batch size = 1, seq length = 2048, embed = 12288, attention heads = 96, depth = 96\n",
      "parallelization: m1 = 4, m2 = 1\n",
      "\n",
      "************** MLP layer estimates **************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>flops_fwd</th>\n",
       "      <th>activation_in_mem</th>\n",
       "      <th>activation_in_other_mem</th>\n",
       "      <th>activation_out_mem</th>\n",
       "      <th>activation_buffer</th>\n",
       "      <th>weights_mem</th>\n",
       "      <th>total_mem_fwd</th>\n",
       "      <th>flops_bwd</th>\n",
       "      <th>activation_grad_mem</th>\n",
       "      <th>weights_grad_mem</th>\n",
       "      <th>total_mem_bwd</th>\n",
       "      <th>comm_bwd</th>\n",
       "      <th>comm_bwd_type</th>\n",
       "      <th>comm_size</th>\n",
       "      <th>comm_fwd</th>\n",
       "      <th>comm_fwd_type</th>\n",
       "      <th>t_comp_fwd</th>\n",
       "      <th>t_mem_fwd</th>\n",
       "      <th>t_comp_bwd</th>\n",
       "      <th>t_mem_bwd</th>\n",
       "      <th>intensity</th>\n",
       "      <th>t_fwd</th>\n",
       "      <th>t_bwd</th>\n",
       "      <th>comm_topology</th>\n",
       "      <th>t_comm_fwd</th>\n",
       "      <th>t_comm_bwd</th>\n",
       "      <th>t_total_fwd</th>\n",
       "      <th>t_total_bwd</th>\n",
       "      <th>frac_t_comm_fwd</th>\n",
       "      <th>frac_t_comm_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fc1</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.302014</td>\n",
       "      <td>0.402678</td>\n",
       "      <td>1.236800</td>\n",
       "      <td>0.100663</td>\n",
       "      <td>0.302014</td>\n",
       "      <td>0.755024</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>reducescatter</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.982293</td>\n",
       "      <td>0.258957</td>\n",
       "      <td>3.964101</td>\n",
       "      <td>0.485546</td>\n",
       "      <td>7.654917</td>\n",
       "      <td>1.982293</td>\n",
       "      <td>3.964101</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>1.982293</td>\n",
       "      <td>4.047987</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.020723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100663</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.064735</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dpr1</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.025166</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.025166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125829</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.080919</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.048551</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.080919</td>\n",
       "      <td>0.048551</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080919</td>\n",
       "      <td>0.048551</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fc2</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.302014</td>\n",
       "      <td>0.402678</td>\n",
       "      <td>1.236800</td>\n",
       "      <td>0.100663</td>\n",
       "      <td>0.302014</td>\n",
       "      <td>0.755024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>reducescatter</td>\n",
       "      <td>1.982293</td>\n",
       "      <td>0.258957</td>\n",
       "      <td>3.964101</td>\n",
       "      <td>0.485546</td>\n",
       "      <td>7.654917</td>\n",
       "      <td>1.982293</td>\n",
       "      <td>3.964101</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.066179</td>\n",
       "      <td>3.964101</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dpr2</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031457</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018874</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>allgather</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.012138</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.012138</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.096024</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.873596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  layer  flops_fwd  activation_in_mem  activation_in_other_mem  \\\n",
       "0   fc1   0.618475           0.050332                 0.000000   \n",
       "1   act   0.000025           0.050332                 0.000000   \n",
       "2  dpr1   0.000025           0.050332                 0.025166   \n",
       "3   fc2   0.618475           0.050332                 0.000000   \n",
       "4  dpr2   0.000006           0.012583                 0.006291   \n",
       "\n",
       "   activation_out_mem  activation_buffer  weights_mem  total_mem_fwd  \\\n",
       "0            0.050332           0.050332     0.302014       0.402678   \n",
       "1            0.050332           0.050332     0.000000       0.100663   \n",
       "2            0.050332           0.025166     0.000000       0.125829   \n",
       "3            0.050332           0.050332     0.302014       0.402678   \n",
       "4            0.012583           0.006291     0.000000       0.031457   \n",
       "\n",
       "   flops_bwd  activation_grad_mem  weights_grad_mem  total_mem_bwd  comm_bwd  \\\n",
       "0   1.236800             0.100663          0.302014       0.755024  0.050332   \n",
       "1   0.000025             0.050332          0.000000       0.100663       NaN   \n",
       "2   0.000025             0.050332          0.000000       0.075497       NaN   \n",
       "3   1.236800             0.100663          0.302014       0.755024       NaN   \n",
       "4   0.000006             0.012583          0.000000       0.018874  0.050332   \n",
       "\n",
       "   comm_bwd_type  comm_size  comm_fwd  comm_fwd_type  t_comp_fwd  t_mem_fwd  \\\n",
       "0  reducescatter        4.0       NaN            NaN    1.982293   0.258957   \n",
       "1            NaN        NaN       NaN            NaN    0.000323   0.064735   \n",
       "2            NaN        NaN       NaN            NaN    0.000323   0.080919   \n",
       "3            NaN        4.0  0.050332  reducescatter    1.982293   0.258957   \n",
       "4      allgather        4.0       NaN            NaN    0.000081   0.020230   \n",
       "\n",
       "   t_comp_bwd  t_mem_bwd  intensity     t_fwd     t_bwd comm_topology  \\\n",
       "0    3.964101   0.485546   7.654917  1.982293  3.964101        nvlink   \n",
       "1    0.000323   0.064735   0.004984  0.064735  0.064735          None   \n",
       "2    0.000323   0.048551   0.003987  0.080919  0.048551          None   \n",
       "3    3.964101   0.485546   7.654917  1.982293  3.964101        nvlink   \n",
       "4    0.000081   0.012138   0.003987  0.020230  0.012138        nvlink   \n",
       "\n",
       "   t_comm_fwd  t_comm_bwd  t_total_fwd  t_total_bwd  frac_t_comm_fwd  \\\n",
       "0    0.000000    0.083886     1.982293     4.047987           0.0000   \n",
       "1    0.000000    0.000000     0.064735     0.064735           0.0000   \n",
       "2    0.000000    0.000000     0.080919     0.048551           0.0000   \n",
       "3    0.083886    0.000000     2.066179     3.964101           0.0406   \n",
       "4    0.000000    0.083886     0.020230     0.096024           0.0000   \n",
       "\n",
       "   frac_t_comm_bwd  \n",
       "0         0.020723  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.873596  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "activation_buffer     17.515414\n",
       "weights_mem           57.986777\n",
       "weights_grad_mem      57.986777\n",
       "flops_fwd            118.752692\n",
       "flops_bwd            237.470956\n",
       "t_total_fwd          404.578111\n",
       "t_total_bwd          789.254319\n",
       "t_comm_fwd             8.053064\n",
       "t_comm_bwd            16.106127\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spend in comms = 0.019904842740418795\n",
      "\n",
      "************** SA layer estimates **************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>flops_fwd</th>\n",
       "      <th>activation_in_mem</th>\n",
       "      <th>activation_in_other_mem</th>\n",
       "      <th>activation_out_mem</th>\n",
       "      <th>activation_buffer</th>\n",
       "      <th>weights_mem</th>\n",
       "      <th>total_mem_fwd</th>\n",
       "      <th>flops_bwd</th>\n",
       "      <th>activation_grad_mem</th>\n",
       "      <th>weights_grad_mem</th>\n",
       "      <th>total_mem_bwd</th>\n",
       "      <th>comm_bwd</th>\n",
       "      <th>comm_bwd_type</th>\n",
       "      <th>comm_size</th>\n",
       "      <th>comm_fwd</th>\n",
       "      <th>comm_fwd_type</th>\n",
       "      <th>t_comp_fwd</th>\n",
       "      <th>t_mem_fwd</th>\n",
       "      <th>t_comp_bwd</th>\n",
       "      <th>t_mem_bwd</th>\n",
       "      <th>intensity</th>\n",
       "      <th>t_fwd</th>\n",
       "      <th>t_bwd</th>\n",
       "      <th>comm_topology</th>\n",
       "      <th>t_comm_fwd</th>\n",
       "      <th>t_comm_bwd</th>\n",
       "      <th>t_total_fwd</th>\n",
       "      <th>t_total_bwd</th>\n",
       "      <th>frac_t_comm_fwd</th>\n",
       "      <th>frac_t_comm_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>layer_norm_1</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.025215</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.025166</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.037798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>allgather</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>0.044768</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100101</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>0.838010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qkv_proj</td>\n",
       "      <td>0.463838</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037749</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.226492</td>\n",
       "      <td>0.314573</td>\n",
       "      <td>0.927575</td>\n",
       "      <td>0.088080</td>\n",
       "      <td>0.226492</td>\n",
       "      <td>0.591397</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>reducescatter</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.486659</td>\n",
       "      <td>0.202298</td>\n",
       "      <td>2.972995</td>\n",
       "      <td>0.380320</td>\n",
       "      <td>7.348870</td>\n",
       "      <td>1.486659</td>\n",
       "      <td>2.972995</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>1.486659</td>\n",
       "      <td>3.056881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logits</td>\n",
       "      <td>0.025669</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.025166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226492</td>\n",
       "      <td>0.051527</td>\n",
       "      <td>0.226492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082273</td>\n",
       "      <td>0.145654</td>\n",
       "      <td>0.165151</td>\n",
       "      <td>0.161838</td>\n",
       "      <td>0.564850</td>\n",
       "      <td>0.145654</td>\n",
       "      <td>0.165151</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145654</td>\n",
       "      <td>0.165151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>softmax</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402653</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.402653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.603980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.258941</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.388411</td>\n",
       "      <td>0.014949</td>\n",
       "      <td>0.258941</td>\n",
       "      <td>0.388411</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258941</td>\n",
       "      <td>0.388411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dropout_softmax</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.100663</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.100663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503316</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.323676</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.194206</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.323676</td>\n",
       "      <td>0.194206</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323676</td>\n",
       "      <td>0.194206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attend</td>\n",
       "      <td>0.025764</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.213910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226492</td>\n",
       "      <td>0.051433</td>\n",
       "      <td>0.213910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082575</td>\n",
       "      <td>0.145654</td>\n",
       "      <td>0.164848</td>\n",
       "      <td>0.275125</td>\n",
       "      <td>0.566927</td>\n",
       "      <td>0.145654</td>\n",
       "      <td>0.275125</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145654</td>\n",
       "      <td>0.275125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v_proj</td>\n",
       "      <td>0.154619</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.075522</td>\n",
       "      <td>0.138437</td>\n",
       "      <td>0.309219</td>\n",
       "      <td>0.062915</td>\n",
       "      <td>0.075522</td>\n",
       "      <td>0.226542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>reducescatter</td>\n",
       "      <td>0.495573</td>\n",
       "      <td>0.089027</td>\n",
       "      <td>0.991086</td>\n",
       "      <td>0.145686</td>\n",
       "      <td>5.566564</td>\n",
       "      <td>0.495573</td>\n",
       "      <td>0.991086</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579459</td>\n",
       "      <td>0.991086</td>\n",
       "      <td>0.144766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dropout</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031457</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018874</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>allgather</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.012138</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.012138</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.096024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>layer_norm_2</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.012583</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.025215</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.025166</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.037798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>allgather</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>0.044768</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>nvlink</td>\n",
       "      <td>0.083886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100101</td>\n",
       "      <td>0.024307</td>\n",
       "      <td>0.838010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             layer  flops_fwd  activation_in_mem  activation_in_other_mem  \\\n",
       "0     layer_norm_1   0.000057           0.012583                 0.000000   \n",
       "1         qkv_proj   0.463838           0.050332                 0.000000   \n",
       "2           logits   0.025669           0.012583                 0.012583   \n",
       "3          softmax   0.000302           0.201327                 0.000000   \n",
       "4  dropout_softmax   0.000101           0.201327                 0.100663   \n",
       "5           attend   0.025764           0.201327                 0.012583   \n",
       "6           v_proj   0.154619           0.012583                 0.000000   \n",
       "7          dropout   0.000006           0.012583                 0.006291   \n",
       "8     layer_norm_2   0.000057           0.012583                 0.000000   \n",
       "\n",
       "   activation_out_mem  activation_buffer  weights_mem  total_mem_fwd  \\\n",
       "0            0.012583           0.012583     0.000049       0.025215   \n",
       "1            0.037749           0.050332     0.226492       0.314573   \n",
       "2            0.201327           0.025166     0.000000       0.226492   \n",
       "3            0.201327           0.201327     0.000000       0.402653   \n",
       "4            0.201327           0.100663     0.000000       0.503316   \n",
       "5            0.012583           0.213910     0.000000       0.226492   \n",
       "6            0.050332           0.012583     0.075522       0.138437   \n",
       "7            0.012583           0.006291     0.000000       0.031457   \n",
       "8            0.012583           0.012583     0.000049       0.025215   \n",
       "\n",
       "   flops_bwd  activation_grad_mem  weights_grad_mem  total_mem_bwd  comm_bwd  \\\n",
       "0   0.000082             0.025166          0.000049       0.037798       NaN   \n",
       "1   0.927575             0.088080          0.226492       0.591397  0.050332   \n",
       "2   0.051527             0.226492          0.000000       0.251658       NaN   \n",
       "3   0.000403             0.402653          0.000000       0.603980       NaN   \n",
       "4   0.000101             0.201327          0.000000       0.301990       NaN   \n",
       "5   0.051433             0.213910          0.000000       0.427819       NaN   \n",
       "6   0.309219             0.062915          0.075522       0.226542       NaN   \n",
       "7   0.000006             0.012583          0.000000       0.018874  0.050332   \n",
       "8   0.000082             0.025166          0.000049       0.037798       NaN   \n",
       "\n",
       "   comm_bwd_type  comm_size  comm_fwd  comm_fwd_type  t_comp_fwd  t_mem_fwd  \\\n",
       "0            NaN        4.0  0.050332      allgather    0.000726   0.016215   \n",
       "1  reducescatter        4.0       NaN            NaN    1.486659   0.202298   \n",
       "2            NaN        NaN       NaN            NaN    0.082273   0.145654   \n",
       "3            NaN        NaN       NaN            NaN    0.003871   0.258941   \n",
       "4            NaN        NaN       NaN            NaN    0.001291   0.323676   \n",
       "5            NaN        NaN       NaN            NaN    0.082575   0.145654   \n",
       "6            NaN        4.0  0.050332  reducescatter    0.495573   0.089027   \n",
       "7      allgather        4.0       NaN            NaN    0.000081   0.020230   \n",
       "8            NaN        4.0  0.050332      allgather    0.000726   0.016215   \n",
       "\n",
       "   t_comp_bwd  t_mem_bwd  intensity     t_fwd     t_bwd comm_topology  \\\n",
       "0    0.001049   0.024307   0.044768  0.016215  0.024307        nvlink   \n",
       "1    2.972995   0.380320   7.348870  1.486659  2.972995        nvlink   \n",
       "2    0.165151   0.161838   0.564850  0.145654  0.165151          None   \n",
       "3    0.005162   0.388411   0.014949  0.258941  0.388411          None   \n",
       "4    0.001291   0.194206   0.003987  0.323676  0.194206          None   \n",
       "5    0.164848   0.275125   0.566927  0.145654  0.275125          None   \n",
       "6    0.991086   0.145686   5.566564  0.495573  0.991086        nvlink   \n",
       "7    0.000081   0.012138   0.003987  0.020230  0.012138        nvlink   \n",
       "8    0.001049   0.024307   0.044768  0.016215  0.024307        nvlink   \n",
       "\n",
       "   t_comm_fwd  t_comm_bwd  t_total_fwd  t_total_bwd  frac_t_comm_fwd  \\\n",
       "0    0.083886    0.000000     0.100101     0.024307         0.838010   \n",
       "1    0.000000    0.083886     1.486659     3.056881         0.000000   \n",
       "2    0.000000    0.000000     0.145654     0.165151         0.000000   \n",
       "3    0.000000    0.000000     0.258941     0.388411         0.000000   \n",
       "4    0.000000    0.000000     0.323676     0.194206         0.000000   \n",
       "5    0.000000    0.000000     0.145654     0.275125         0.000000   \n",
       "6    0.083886    0.000000     0.579459     0.991086         0.144766   \n",
       "7    0.000000    0.083886     0.020230     0.096024         0.000000   \n",
       "8    0.083886    0.000000     0.100101     0.024307         0.838010   \n",
       "\n",
       "   frac_t_comm_bwd  \n",
       "0         0.000000  \n",
       "1         0.027442  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  \n",
       "5         0.000000  \n",
       "6         0.000000  \n",
       "7         0.873596  \n",
       "8         0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "activation_buffer     61.001957\n",
       "weights_mem           29.002826\n",
       "weights_grad_mem      29.002826\n",
       "flops_fwd             64.359476\n",
       "flops_bwd            128.680906\n",
       "t_total_fwd          303.405760\n",
       "t_total_bwd          500.687843\n",
       "t_comm_fwd            24.159191\n",
       "t_comm_bwd            16.106127\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spend in comms = 0.07962667242415632\n"
     ]
    }
   ],
   "source": [
    "### model\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "b = 1\n",
    "patch = 16\n",
    "ih = 720\n",
    "iw = 1440 \n",
    "l = 2048 #ih // patch * iw // patch\n",
    "e = 12288\n",
    "f = 4 * e\n",
    "h = 96\n",
    "depth = 96\n",
    "fp32_sz = 4E-9\n",
    "fp16_sz = 2E-9\n",
    "int_sz = 1E-9\n",
    "flops_units = 1E-12 # teraflops\n",
    "\n",
    "print(\"model: batch size = {}, seq length = {}, embed = {}, attention heads = {}, depth = {}\".format(b, l, e, h, depth))\n",
    "\n",
    "### model parallelism\n",
    "parallelism = {'m1': 4,\n",
    "               'm2': 1}\n",
    "### system configs\n",
    "system = {'matrix_flops_fp16': 312,\n",
    "          'vector_flops_fp32': 19.5,\n",
    "          'vector_flops_fp16': 78,\n",
    "          'hbm_bandwidth': 1555,\n",
    "          'nvlink_bandwidth': 600,\n",
    "          'ib_bandwidth': 100,\n",
    "          'nvlink_size': 4}\n",
    "print(\"parallelization: m1 = {}, m2 = {}\".format(parallelism['m1'], parallelism['m2']))\n",
    "\n",
    "# MLP\n",
    "df_mlp = MLP_estimates(b, l, e, f, depth, element_size=fp16_sz, mask_element_size=int_sz, flops_units=flops_units, \n",
    "                       parallelism=parallelism, system=system)\n",
    "cols = df_mlp.columns.tolist()\n",
    "cols.remove('layer')\n",
    "cols = ['layer'] + cols\n",
    "\n",
    "# self attention\n",
    "df_sa = self_attention_estimates(b, l, e, h, element_size=fp16_sz, mask_element_size=int_sz, flops_units=flops_units, \n",
    "                                 parallelism=parallelism, system=system)\n",
    "\n",
    "# sum these columns (mem in buffer: activation buffers, weights, weights_grads, total flops, timings)\n",
    "layer_track_cols = ['activation_buffer', 'weights_mem', \n",
    "                    'weights_grad_mem', 'flops_fwd', 'flops_bwd', \n",
    "                    't_total_fwd', 't_total_bwd', 't_comm_fwd', 't_comm_bwd']\n",
    "\n",
    "print('\\n************** MLP layer estimates **************\\n')\n",
    "display(df_mlp[cols])\n",
    "display(df_mlp[layer_track_cols].sum() * depth)\n",
    "t_f = df_mlp['t_total_fwd'].sum()\n",
    "t_c = df_mlp['t_comm_fwd'].sum()\n",
    "print('time spend in comms = {}'.format(t_c / t_f))\n",
    "\n",
    "print('\\n************** SA layer estimates **************\\n')\n",
    "display(df_sa[cols])\n",
    "display(df_sa[layer_track_cols].sum() * depth)\n",
    "t_f = df_sa['t_total_fwd'].sum()\n",
    "t_c = df_sa['t_comm_fwd'].sum()\n",
    "print('time spend in comms = {}'.format(t_c / t_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756616ec-f3e9-493b-b52d-a20c8e64a7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0!=[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8d1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85988d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
