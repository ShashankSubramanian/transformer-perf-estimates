{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21213552-eba3-426b-ae8b-2529d6c8ddf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be58a90-081f-48f5-adec-be236a190106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matmul import linear_estimates\n",
    "from norm import layer_norm_estimates\n",
    "from attention import logit_estimates\n",
    "from pointwise import softmax_estimates, dropout_estimates, nonlinear_act_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36c909a-afa6-46ab-b887-e7182bd35c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_layer_estimates(summary, layer=\"dnn\", mult_factor=1):\n",
    "    ''' be careful here in what you add up: only need some tensors for bwd\n",
    "        but some layers may have large immediate activations as well.\n",
    "        adds individual layer estimate '''\n",
    "    estimate = []\n",
    "    layer_estimate = {'layer': layer,\n",
    "                      'activation_mem': 0,\n",
    "                      'weights_mem': 0,\n",
    "                      'grad_mem': 0,\n",
    "                      'flops_fwd': 0,\n",
    "                      'flops_bwd': 0,\n",
    "                      'comm_fwd_allreduce': 0,\n",
    "                      'comm_bwd_allreduce': 0,\n",
    "                      'comm_fwd_allgather': 0,\n",
    "                      'comm_bwd_allgather': 0,\n",
    "                      'comm_fwd_reducescatter': 0,\n",
    "                      'comm_bwd_reducescatter': 0}\n",
    "    \n",
    "    for depth, stats in enumerate(summary):\n",
    "        layer_estimate['activation_mem'] += stats['activation_buffer']\n",
    "        layer_estimate['weights_mem'] += stats['weights_mem']\n",
    "        layer_estimate['grad_mem'] += stats['weights_grad_mem']\n",
    "        layer_estimate['flops_fwd'] += stats['flops_fwd']\n",
    "        layer_estimate['flops_bwd'] += stats['flops_bwd']\n",
    "        \n",
    "        # count comms\n",
    "        layer_estimate['comm_fwd_allreduce'] += stats['comm_fwd_allreduce'] if 'comm_fwd_allreduce' in stats else 0\n",
    "        layer_estimate['comm_bwd_allreduce'] += stats['comm_bwd_allreduce'] if 'comm_bwd_allreduce' in stats else 0\n",
    "        \n",
    "        layer_estimate['comm_fwd_allgather'] += stats['comm_fwd_allgather'] if 'comm_fwd_allgather' in stats else 0\n",
    "        layer_estimate['comm_bwd_allgather'] += stats['comm_bwd_allgather'] if 'comm_bwd_allgather' in stats else 0\n",
    "        \n",
    "        layer_estimate['comm_fwd_reducescatter'] += stats['comm_fwd_reducescatter'] if 'comm_fwd_reducescatter' in stats else 0\n",
    "        layer_estimate['comm_bwd_reducescatter'] += stats['comm_bwd_reducescatter'] if 'comm_bwd_reducescatter' in stats else 0\n",
    "        \n",
    "    layer_estimate['total_comm'] = layer_estimate['comm_fwd_allreduce'] + layer_estimate['comm_bwd_allreduce']\n",
    "    layer_estimate['total_comm'] += layer_estimate['comm_fwd_allgather'] + layer_estimate['comm_bwd_allgather']\n",
    "    layer_estimate['total_comm'] += layer_estimate['comm_fwd_reducescatter'] + layer_estimate['comm_bwd_reducescatter']\n",
    "    estimate.append(layer_estimate)\n",
    "    total_estimate = {}\n",
    "    for k, v in layer_estimate.items():\n",
    "        if k == 'layer':\n",
    "            total_estimate[k] = v + \"_depth\"\n",
    "        else:\n",
    "            total_estimate[k] = v * mult_factor\n",
    "    estimate.append(total_estimate)\n",
    "    return estimate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bc19b5-3714-4e49-9429-a81a2011204d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### nn modules ###\n",
    "def MLP_estimates(b, l, e, f, depth, element_size=4E-6, mask_element_size=1E-6, parallelism={'m1': 1, 'm2': 1}):\n",
    "    \"\"\"\n",
    "    MLP layer estimates\n",
    "    parameters: b: batch size\n",
    "                l: seq length\n",
    "                e: embedding dim\n",
    "                f: hidden dim\n",
    "                element_size: in MB\n",
    "                mask_element_size: in MB (for dropout)\n",
    "    \n",
    "    tensor shapes: input tensor: (b,l,e)\n",
    "                   output tensor: (b,l,e)\n",
    "                   \n",
    "    layer arithmetic: \n",
    "        forward pass: \n",
    "             X = XW + b\n",
    "             (b,l,f) = (b,l,e) * (e,f) + (1,f)\n",
    "             X = nonlinear(X)\n",
    "             (b,l,f) = (b,l,f)\n",
    "             X = dropout(X)\n",
    "             (b,l,f) = (b,l,f) * (b,l,f) [random mask]\n",
    "             X = linear(X)\n",
    "             (b,l,e) = (b,l,f) * (f,e) + (1,e)\n",
    "             X = dropout(X)\n",
    "             (b,l,e) = (b,l,e) * (b,l,e) [random mask]\n",
    "            \n",
    "        backward pass:\n",
    "             chain rule\n",
    "             \n",
    "    parallelism:\n",
    "            X = XW + b\n",
    "            (b,l,f/m) = (b,l,e) * (e,f/m) + (1,f/m)\n",
    "            X = nonlinear(X)\n",
    "            (b,l,f/m) = (b,l,f/m)\n",
    "            X = dropout(X)\n",
    "            (b,l,f/m) = (b,l,f/m) * (b,l,f/m) [random mask]\n",
    "            X = linear(X)\n",
    "            (b,l,e/m) = (b,l,f/m) * (f/m,e) + (1,e)\n",
    "            X = dropout(X)\n",
    "            (b,l,e) = (b,l,e) * (b,l,e) [random mask]\n",
    "            \n",
    "    comments: \n",
    "    \"\"\"\n",
    "    \n",
    "    summary = []\n",
    "    \n",
    "    m1 = parallelism['m1']\n",
    "    m2 = 1 # parallelism['m2'] # not used in 1D parallelism (set to 1)\n",
    "        \n",
    "    stats = linear_estimates(b, l, e, f // m1, element_size=element_size, has_bias=True)\n",
    "    stats[\"layer\"] = \"fc1\"   \n",
    "    # sync/comm layers\n",
    "    # no fwd comms\n",
    "    stats[\"comm_bwd_reducescatter\"] = (b * l * e) * element_size # bwd comms for partial sums of b,l,e\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = nonlinear_act_estimates(b, l, f // m1, element_size=element_size)\n",
    "    stats[\"layer\"] = \"act\"\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = dropout_estimates(b, l, f // m1, element_size=element_size, mask_element_size=mask_element_size)\n",
    "    stats[\"layer\"] = \"dpr1\"\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = linear_estimates(b, l, f // m1, e, element_size=element_size, has_bias=True)\n",
    "    stats[\"layer\"] = \"fc2\"\n",
    "    # sync/comm layers\n",
    "    # no bwd comms\n",
    "    stats[\"comm_fwd_reducescatter\"] =  (b * l * e) * element_size # fwd comms for partial sums of b,l,e\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = dropout_estimates(b, l // m1, e, element_size=element_size, mask_element_size=mask_element_size)\n",
    "    stats[\"layer\"] = \"dpr2\"\n",
    "    summary.append(stats)\n",
    "    \n",
    "    estimate = compute_layer_estimates(summary, layer=\"mlp\", mult_factor=depth)\n",
    "    \n",
    "    return pd.DataFrame(summary), pd.DataFrame(estimate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2d41fa-3acd-4db3-b89d-da5cc2384945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_estimates(b, l, e, h, element_size=4E-6, mask_element_size=1E-6, parallelism={'m1': 1, 'm2': 1}):\n",
    "    \"\"\"\n",
    "    dropout layer estimates\n",
    "    parameters: b: batch size\n",
    "                l: seq length\n",
    "                e: embedding dim/hidden dim\n",
    "                h: number of attention heads\n",
    "                element_size: in MB\n",
    "    \n",
    "    tensor shapes: input tensor: (b,l,e)\n",
    "                   output tensor: (b,l,e)\n",
    "                   \n",
    "    layer arithmetic: \n",
    "        define: q = e/h\n",
    "        forward pass: \n",
    "             X = norm(X)\n",
    "             Q = XW, K = XW, V = XW\n",
    "             (b,l,h,q,3) = (b,l,e) * (e,3hq)\n",
    "             A = QK'/sqrt(q)\n",
    "             (b,h,l,l) = (b,h,l,q) * (b,h,q,l)\n",
    "             A = softmax(A)\n",
    "             (b,h,l,l) = (b,h,l,l)\n",
    "             A = dpr(A)\n",
    "             Y = AV\n",
    "             (b,h,l,q) = (b,h,l,l) * (b,h,l,q)\n",
    "             Y = VW\n",
    "             (b,l,e) = (b,l,hq) * (hq,e)\n",
    "             Y = dpr(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "             Y = norm(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "             \n",
    "        backward pass:\n",
    "             chain rule\n",
    "             \n",
    "        parallelism:\n",
    "             X = norm(X)\n",
    "             Q = XW, K = XW, V = XW\n",
    "             (b,l,h/m,q,3) = (b,l,e) * (e,3hq/m)\n",
    "             A = QK'/sqrt(q)\n",
    "             (b,h/m,l,l) = (b,h/m,l,q) * (b,h/m,q,l)\n",
    "             A = softmax(A)\n",
    "             (b,h/m,l,l) = (b,h/m,l,l)\n",
    "             A = dpr(A)\n",
    "             (b,h/m,l,l) = (b,h/m,l,l)\n",
    "             Y = AV\n",
    "             (b,h/m,l,q) = (b,h/m,l,l) * (b,h/m,l,q)\n",
    "             Y = VW\n",
    "             (b,l,e) = (b,l,hq/m) * (hq/m,e)\n",
    "             Y = dpr(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "             Y = norm(Y)\n",
    "             (b,l,e) = (b,l,e)\n",
    "            \n",
    "    \n",
    "    comments: \n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    q = e // h\n",
    "    \n",
    "    m1 = parallelism['m1']\n",
    "    m2 = 1 #parallelism['m2'] # 1D parallelism for now\n",
    "    \n",
    "    stats = layer_norm_estimates(b, l // m1, e, element_size=element_size)\n",
    "    stats[\"layer\"] = \"layer_norm_1\"\n",
    "    # sync/comm layers\n",
    "    stats[\"comm_fwd_allgather\"] = (b * l * e) * element_size # all gather for the next op\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = linear_estimates(b, l, e, (3*e) // m1, element_size=element_size, has_bias=False)\n",
    "    stats[\"layer\"] = \"qkv_proj\"\n",
    "    # sync/comm layers: no fwd coms here\n",
    "    stats[\"comm_bwd_reducescatter\"] = (b * l * e) * element_size # reduce scatter before going to ln: TODO check?\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = logit_estimates(b, l, e, h // m1, element_size=element_size)\n",
    "    stats[\"layer\"] = \"logits\"\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = softmax_estimates(b, l, h // m1, element_size=element_size)\n",
    "    stats[\"layer\"] = \"softmax\"\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = dropout_estimates(b, l, (l*h) // m1, element_size=element_size, mask_element_size=mask_element_size)\n",
    "    stats[\"layer\"] = \"dropout_softmax\"\n",
    "    summary.append(stats)\n",
    "\n",
    "    # TODO/Sh: this is incorrect, it should be a different function\n",
    "    stats = linear_estimates((b*h) // m1, l, l, q, element_size=element_size, has_bias=False)\n",
    "    stats[\"layer\"] = \"attend\"\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = linear_estimates(b, l, (h*q) // m1, e, element_size=element_size, has_bias=True)\n",
    "    stats[\"layer\"] = \"v_proj\"\n",
    "    # sync/comm layers\n",
    "    stats[\"comm_fwd_reducescatter\"] = (b * l * e) * element_size # fwd comms for partial sums of b,l,e\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = dropout_estimates(b, l // m1, e, element_size=element_size, mask_element_size=mask_element_size)\n",
    "    stats[\"layer\"] = \"dropout\"\n",
    "    summary.append(stats)\n",
    "    \n",
    "    stats = layer_norm_estimates(b, l // m1, e, element_size=element_size)\n",
    "    stats[\"layer\"] = \"layer_norm_2\"\n",
    "    # sync/comm layers\n",
    "    stats[\"comm_fwd_allgather\"] = (b * l * e) * element_size # all gather for the next op\n",
    "    summary.append(stats)\n",
    "    \n",
    "    estimate = compute_layer_estimates(summary, layer=\"self-attn\", mult_factor=depth)\n",
    "    \n",
    "    return pd.DataFrame(summary), pd.DataFrame(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625fd0e1-403d-4fd5-a5dc-472832d0b039",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: batch size = 1, seq length = 64800, embed = 1024, attention heads = 8\n",
      "parallelization: m1 = 4, m2 = 1\n"
     ]
    }
   ],
   "source": [
    "### model\n",
    "b = 1\n",
    "patch = 4\n",
    "ih = 720\n",
    "iw = 1440 \n",
    "l = ih // patch * iw // patch\n",
    "e = 1024\n",
    "f = 4 * e\n",
    "h = 8\n",
    "depth = 12\n",
    "fp32_sz = 4E-9\n",
    "fp16_sz = 2E-9\n",
    "int_sz = 1E-9\n",
    "\n",
    "print(\"model: batch size = {}, seq length = {}, embed = {}, attention heads = {}\".format(b, l, e, h))\n",
    "\n",
    "### model parallelism\n",
    "parallelism = {'m1': 4,\n",
    "               'm2': 1}\n",
    "\n",
    "print(\"parallelization: m1 = {}, m2 = {}\".format(parallelism['m1'], parallelism['m2']))\n",
    "\n",
    "# MLP\n",
    "df_mlp, df_mlp_est = MLP_estimates(b, l, e, f, depth, element_size=fp16_sz, mask_element_size=int_sz, parallelism=parallelism)\n",
    "cols = df_mlp.columns.tolist()\n",
    "cols.remove('layer')\n",
    "cols = ['layer'] + cols\n",
    "\n",
    "# self attention\n",
    "df_sa, df_sa_est = self_attention_estimates(b, l, e, h, element_size=fp16_sz, mask_element_size=int_sz,parallelism=parallelism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3901ce-41cc-41c5-9950-ba04e43a58e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** MLP layer estimates **************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>flops_fwd</th>\n",
       "      <th>activation_in_mem</th>\n",
       "      <th>activation_in_other_mem</th>\n",
       "      <th>activation_out_mem</th>\n",
       "      <th>activation_buffer</th>\n",
       "      <th>weights_mem</th>\n",
       "      <th>total_mem_fwd</th>\n",
       "      <th>flops_bwd</th>\n",
       "      <th>activation_grad_mem</th>\n",
       "      <th>weights_grad_mem</th>\n",
       "      <th>total_mem_bwd</th>\n",
       "      <th>comm_bwd_reducescatter</th>\n",
       "      <th>comm_fwd_reducescatter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fc1</td>\n",
       "      <td>135.895450</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.267520</td>\n",
       "      <td>271.789851</td>\n",
       "      <td>0.265421</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.267520</td>\n",
       "      <td>0.13271</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265421</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dpr1</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331776</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fc2</td>\n",
       "      <td>135.895450</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.267520</td>\n",
       "      <td>271.789851</td>\n",
       "      <td>0.265421</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.267520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dpr2</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082944</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  layer   flops_fwd  activation_in_mem  activation_in_other_mem  \\\n",
       "0   fc1  135.895450           0.132710                 0.000000   \n",
       "1   act    0.066355           0.132710                 0.000000   \n",
       "2  dpr1    0.066355           0.132710                 0.066355   \n",
       "3   fc2  135.895450           0.132710                 0.000000   \n",
       "4  dpr2    0.016589           0.033178                 0.016589   \n",
       "\n",
       "   activation_out_mem  activation_buffer  weights_mem  total_mem_fwd  \\\n",
       "0            0.132710           0.132710     0.002099       0.267520   \n",
       "1            0.132710           0.132710     0.000000       0.265421   \n",
       "2            0.132710           0.066355     0.000000       0.331776   \n",
       "3            0.132710           0.132710     0.002099       0.267520   \n",
       "4            0.033178           0.016589     0.000000       0.082944   \n",
       "\n",
       "    flops_bwd  activation_grad_mem  weights_grad_mem  total_mem_bwd  \\\n",
       "0  271.789851             0.265421          0.002099       0.267520   \n",
       "1    0.066355             0.132710          0.000000       0.132710   \n",
       "2    0.066355             0.132710          0.000000       0.132710   \n",
       "3  271.789851             0.265421          0.002099       0.267520   \n",
       "4    0.016589             0.033178          0.000000       0.033178   \n",
       "\n",
       "   comm_bwd_reducescatter  comm_fwd_reducescatter  \n",
       "0                 0.13271                     NaN  \n",
       "1                     NaN                     NaN  \n",
       "2                     NaN                     NaN  \n",
       "3                     NaN                 0.13271  \n",
       "4                     NaN                     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>activation_mem</th>\n",
       "      <th>weights_mem</th>\n",
       "      <th>grad_mem</th>\n",
       "      <th>flops_fwd</th>\n",
       "      <th>flops_bwd</th>\n",
       "      <th>comm_fwd_allreduce</th>\n",
       "      <th>comm_bwd_allreduce</th>\n",
       "      <th>comm_fwd_allgather</th>\n",
       "      <th>comm_bwd_allgather</th>\n",
       "      <th>comm_fwd_reducescatter</th>\n",
       "      <th>comm_bwd_reducescatter</th>\n",
       "      <th>total_comm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.481075</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>271.940198</td>\n",
       "      <td>543.729000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.265421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mlp_depth</td>\n",
       "      <td>5.772902</td>\n",
       "      <td>0.050381</td>\n",
       "      <td>0.050381</td>\n",
       "      <td>3263.282381</td>\n",
       "      <td>6524.748005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.592525</td>\n",
       "      <td>1.592525</td>\n",
       "      <td>3.185050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       layer  activation_mem  weights_mem  grad_mem    flops_fwd    flops_bwd  \\\n",
       "0        mlp        0.481075     0.004198  0.004198   271.940198   543.729000   \n",
       "1  mlp_depth        5.772902     0.050381  0.050381  3263.282381  6524.748005   \n",
       "\n",
       "   comm_fwd_allreduce  comm_bwd_allreduce  comm_fwd_allgather  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "\n",
       "   comm_bwd_allgather  comm_fwd_reducescatter  comm_bwd_reducescatter  \\\n",
       "0                   0                0.132710                0.132710   \n",
       "1                   0                1.592525                1.592525   \n",
       "\n",
       "   total_comm  \n",
       "0    0.265421  \n",
       "1    3.185050  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** SA layer estimates **************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>flops_fwd</th>\n",
       "      <th>activation_in_mem</th>\n",
       "      <th>activation_in_other_mem</th>\n",
       "      <th>activation_out_mem</th>\n",
       "      <th>activation_buffer</th>\n",
       "      <th>weights_mem</th>\n",
       "      <th>total_mem_fwd</th>\n",
       "      <th>flops_bwd</th>\n",
       "      <th>activation_grad_mem</th>\n",
       "      <th>weights_grad_mem</th>\n",
       "      <th>total_mem_bwd</th>\n",
       "      <th>comm_bwd_reducescatter</th>\n",
       "      <th>comm_fwd_reducescatter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>layer_norm_1</td>\n",
       "      <td>0.149299</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.066359</td>\n",
       "      <td>0.215590</td>\n",
       "      <td>0.099533</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.099537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qkv_proj</td>\n",
       "      <td>101.871821</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099533</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.233816</td>\n",
       "      <td>203.776033</td>\n",
       "      <td>0.232243</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>0.13271</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logits</td>\n",
       "      <td>8591.235840</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>0.265421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.061581</td>\n",
       "      <td>17199.135130</td>\n",
       "      <td>17.061581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.061581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>softmax</td>\n",
       "      <td>25.194110</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.592320</td>\n",
       "      <td>33.592190</td>\n",
       "      <td>33.592320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.592320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dropout_softmax</td>\n",
       "      <td>8.398080</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>8.398080</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>8.398080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.990400</td>\n",
       "      <td>8.398080</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attend</td>\n",
       "      <td>2149.891891</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>16.796160</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>16.845926</td>\n",
       "      <td>4291.410586</td>\n",
       "      <td>16.829338</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>16.845927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v_proj</td>\n",
       "      <td>33.973862</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.166414</td>\n",
       "      <td>67.997229</td>\n",
       "      <td>0.165888</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.166414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dropout</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082944</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>layer_norm_2</td>\n",
       "      <td>0.149299</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.033178</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.066359</td>\n",
       "      <td>0.215590</td>\n",
       "      <td>0.099533</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.099537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             layer    flops_fwd  activation_in_mem  activation_in_other_mem  \\\n",
       "0     layer_norm_1     0.149299           0.033178                 0.000000   \n",
       "1         qkv_proj   101.871821           0.132710                 0.000000   \n",
       "2           logits  8591.235840           0.132710                 0.132710   \n",
       "3          softmax    25.194110          16.796160                 0.000000   \n",
       "4  dropout_softmax     8.398080          16.796160                 8.398080   \n",
       "5           attend  2149.891891          16.796160                 0.000000   \n",
       "6           v_proj    33.973862           0.033178                 0.000000   \n",
       "7          dropout     0.016589           0.033178                 0.016589   \n",
       "8     layer_norm_2     0.149299           0.033178                 0.000000   \n",
       "\n",
       "   activation_out_mem  activation_buffer  weights_mem  total_mem_fwd  \\\n",
       "0            0.033178           0.033178     0.000004       0.066359   \n",
       "1            0.099533           0.132710     0.001573       0.233816   \n",
       "2           16.796160           0.265421     0.000000      17.061581   \n",
       "3           16.796160          16.796160     0.000000      33.592320   \n",
       "4           16.796160           8.398080     0.000000      41.990400   \n",
       "5            0.033178          16.796160     0.016589      16.845926   \n",
       "6            0.132710           0.033178     0.000526       0.166414   \n",
       "7            0.033178           0.016589     0.000000       0.082944   \n",
       "8            0.033178           0.033178     0.000004       0.066359   \n",
       "\n",
       "      flops_bwd  activation_grad_mem  weights_grad_mem  total_mem_bwd  \\\n",
       "0      0.215590             0.099533          0.000004       0.099537   \n",
       "1    203.776033             0.232243          0.001574       0.233818   \n",
       "2  17199.135130            17.061581          0.000000      17.061581   \n",
       "3     33.592190            33.592320          0.000000      33.592320   \n",
       "4      8.398080            16.796160          0.000000      16.796160   \n",
       "5   4291.410586            16.829338          0.016589      16.845927   \n",
       "6     67.997229             0.165888          0.000526       0.166414   \n",
       "7      0.016589             0.033178          0.000000       0.033178   \n",
       "8      0.215590             0.099533          0.000004       0.099537   \n",
       "\n",
       "   comm_bwd_reducescatter  comm_fwd_reducescatter  \n",
       "0                     NaN                     NaN  \n",
       "1                 0.13271                     NaN  \n",
       "2                     NaN                     NaN  \n",
       "3                     NaN                     NaN  \n",
       "4                     NaN                     NaN  \n",
       "5                     NaN                     NaN  \n",
       "6                     NaN                 0.13271  \n",
       "7                     NaN                     NaN  \n",
       "8                     NaN                     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>activation_mem</th>\n",
       "      <th>weights_mem</th>\n",
       "      <th>grad_mem</th>\n",
       "      <th>flops_fwd</th>\n",
       "      <th>flops_bwd</th>\n",
       "      <th>comm_fwd_allreduce</th>\n",
       "      <th>comm_bwd_allreduce</th>\n",
       "      <th>comm_fwd_allgather</th>\n",
       "      <th>comm_bwd_allgather</th>\n",
       "      <th>comm_fwd_reducescatter</th>\n",
       "      <th>comm_bwd_reducescatter</th>\n",
       "      <th>total_comm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>self-attn</td>\n",
       "      <td>42.504653</td>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>10910.880792</td>\n",
       "      <td>21804.757015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265421</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.132710</td>\n",
       "      <td>0.530842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>self-attn_depth</td>\n",
       "      <td>510.055834</td>\n",
       "      <td>0.224354</td>\n",
       "      <td>0.224376</td>\n",
       "      <td>130930.569504</td>\n",
       "      <td>261657.084185</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.185050</td>\n",
       "      <td>0</td>\n",
       "      <td>1.592525</td>\n",
       "      <td>1.592525</td>\n",
       "      <td>6.370099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             layer  activation_mem  weights_mem  grad_mem      flops_fwd  \\\n",
       "0        self-attn       42.504653     0.018696  0.018698   10910.880792   \n",
       "1  self-attn_depth      510.055834     0.224354  0.224376  130930.569504   \n",
       "\n",
       "       flops_bwd  comm_fwd_allreduce  comm_bwd_allreduce  comm_fwd_allgather  \\\n",
       "0   21804.757015                   0                   0            0.265421   \n",
       "1  261657.084185                   0                   0            3.185050   \n",
       "\n",
       "   comm_bwd_allgather  comm_fwd_reducescatter  comm_bwd_reducescatter  \\\n",
       "0                   0                0.132710                0.132710   \n",
       "1                   0                1.592525                1.592525   \n",
       "\n",
       "   total_comm  \n",
       "0    0.530842  \n",
       "1    6.370099  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print('\\n************** MLP layer estimates **************\\n')\n",
    "display(df_mlp[cols])\n",
    "display(df_mlp_est)\n",
    "\n",
    "print('\\n************** SA layer estimates **************\\n')\n",
    "\n",
    "display(df_sa[cols])\n",
    "display(df_sa_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d7e66-694e-47ab-b213-b1be0545a94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283831b7-852a-432d-873d-edda311fbaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.13.1",
   "language": "python",
   "name": "pytorch-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
